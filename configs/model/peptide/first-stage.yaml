_target_: src.models.composites.first_stage.peptide.Wrapper
_recursive_: False
monitor: val/loss
interval: step
compile: False
shift: ${shift}
scale: ${scale}

loss:
  _target_: src.models.composites.first_stage.peptide.Loss
  loss_pos_weight: 1
  loss_pos:
    _target_: src.modules.losses.MaskedMSELoss

  loss_pos_frame_weight: 0
  loss_pos_frame:
    _target_: src.modules.losses.MaskedMSELoss

  loss_inter_distance_weight: 1.0
  loss_inter_distance:
    _target_: src.modules.losses.InterDistanceLoss

  loss_res_type_weight: 0.01
  loss_res_type:
    _target_: torch.nn.CrossEntropyLoss

  loss_torsion_weight: 0.0
  loss_torsion:
    _target_: src.modules.losses.MaskedCosineLoss

  loss_norm_weight: 0.0
  loss_norm:
    _target_: src.modules.losses.MaskedNormLoss

backbone:
  _target_: src.models.composites.first_stage.peptide.Backbone
  dim_input: 256
  dim_latent: 96
  max_res: 10
  act:
    _target_: src.modules.torch_modules.GELU
    _partial_: True

  embedding_entity:
    _target_: src.modules.entity_embeddings.EntityEmbeddingOrthogonal
    n_entiy_embeddings: ${num_entities}
    embedding_dim: 128
    max_norm: 1
    requires_grad: False

  embedding_res:
    _target_: torch.nn.Embedding
    num_embeddings: 20
    embedding_dim: 64
    max_norm: 1

  encoder:
    _target_: src.models.components.encoder.Encoder
    _partial_: True
    dim_input: ${model.backbone.dim_input}
    dim_latent: ${model.backbone.dim_latent}
    dim_head_cross: 16
    dim_head_latent: 16
    num_latents: 2
    num_head_cross: 2
    num_head_latent: 2
    num_block_attn: 1
    num_block_cross: 1
    qk_norm: True
    dropout_latent: 0.0
    act: ${model.backbone.act}

  decoder:
    _target_: src.models.components.decoder.DecoderQuerySplitter
    _partial_: True
    outputs:
      atom14_pos: 42 # 3 * 14 (Atom14 representation + 3 coordinates)
      aatype: 20 # 20 (20 amino acids)
    dim_latent: ${model.backbone.encoder.dim_latent}
    dim_query: 128
    dim_head_cross: 16
    dim_head_latent: 16
    num_head_cross: 2
    num_head_latent: 2
    num_block_cross: 0
    num_block_attn: 1
    dropout_query: 0.1
    num_split: 8
    qk_norm: ${model.backbone.encoder.qk_norm}
    act: ${model.backbone.act}

optimizer:
  _target_: torch.optim.AdamW
  _partial_: True
  lr: 1e-3

scheduler:
  _target_: src.modules.schedulers.LinearWarmupCosineAnnealingLR
  _partial_: True
  warmup_epochs: 0
  max_epochs: ${trainer.max_epochs}
  min_lr: 1e-7
